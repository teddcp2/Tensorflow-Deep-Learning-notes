{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\r\n",
    "\r\n",
    "- decides which neuron to pass  to next layer\r\n",
    "- brings up the non-linearity relationship between input and output\r\n",
    "\r\n",
    "- [Why activation function is required ?](https://stats.stackexchange.com/a/236386/279934) and [link 2](https://stackoverflow.com/a/9783865/12210002)\r\n",
    "- [non-linearity with Neural Network](https://stats.stackexchange.com/a/335972/279934)\r\n",
    "- [how to choose activation functions](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)\r\n",
    "- [how to choose an activation function](https://ai.stackexchange.com/a/7089/47710)\r\n",
    "- [Tensorflow Activation list](https://www.tensorflow.org/api_docs/python/tf/keras/activations)\r\n",
    "- [tanh vs sigmoid activation function](https://stats.stackexchange.com/a/250456/279934)\r\n",
    "- [relu is better than sigmoid](https://stats.stackexchange.com/a/211359/279934)\r\n",
    "- [different cost functions](https://stats.stackexchange.com/a/154880/279934)\r\n",
    "  \r\n",
    "## detailed Articles\r\n",
    "\r\n",
    "- [detailed article](https://machinelearningknowledge.ai/activation-functions-neural-network/)\r\n",
    "- [kERAS ACTUVATION FUNCTION LIST](https://machinelearningknowledge.ai/keras-activation-layers-ultimate-guide-for-beginners/)\r\n",
    "- [Article](https://machinelearningknowledge.ai/pytorch-activation-functions-relu-leaky-relu-sigmoid-tanh-and-softmax/)\r\n",
    "- [Article 1](https://mlfromscratch.com/activation-functions-explained/#/)\r\n",
    "- [cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\r\n",
    "- [aritcle 2](https://iq.opengenus.org/types-of-activation-function/)\r\n",
    "- [list of functions](https://stats.stackexchange.com/a/154877/279934)\r\n",
    "- [list of functions 2 (wikipedia)](https://en.wikipedia.org/wiki/Activation_function)\r\n",
    "- [artilce 3](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/)\r\n",
    "- [Keras activation details](https://keras.io/api/layers/activations/)\r\n",
    "- [article-4](https://www.machinecurve.com/index.php/2019/09/04/relu-sigmoid-and-tanh-todays-most-used-activation-functions/)\r\n",
    "- [article 5](https://www.v7labs.com/blog/neural-networks-activation-functions)\r\n",
    "\r\n",
    "## List of functions\r\n",
    "\r\n",
    "- RELU\r\n",
    "  - [How does RELU behave for z<0 ?](https://stats.stackexchange.com/a/308706/279934)\r\n",
    "  - [Dying neuron Problem](https://datascience.stackexchange.com/a/9431/94070)\r\n",
    "  - [how relu is non-linear](https://datascience.stackexchange.com/a/26476/94070), [link 2](https://stats.stackexchange.com/a/299933/279934) and [link3](https://stats.stackexchange.com/a/141978/279934)\r\n",
    "  - [relu vs sigmoid and tanh](https://www.machinecurve.com/index.php/2019/09/04/relu-sigmoid-and-tanh-todays-most-used-activation-functions/)\r\n",
    "  - [RELU vs ELU vs SELU](https://stats.stackexchange.com/a/489288/279934)\r\n",
    "  - [relu description](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)\r\n",
    "  - [swish is better than Relu](https://www.machinecurve.com/index.php/2019/05/30/why-swish-could-perform-better-than-relu/)\r\n",
    "  - [Leaky Relu](https://www.machinecurve.com/index.php/2019/10/15/leaky-relu-improving-traditional-relu/)\r\n",
    "  - \r\n",
    "\r\n",
    "- softmax\r\n",
    "  - [article 1](https://machinelearningmastery.com/softmax-activation-function-with-python/)\r\n",
    "  - \r\n",
    "\r\n",
    "## Articles\r\n",
    "\r\n",
    "- [activation functions cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\r\n",
    "\r\n",
    "- [detailed explanation-1](https://machinelearningknowledge.ai/pytorch-activation-functions-relu-leaky-relu-sigmoid-tanh-and-softmax/)\r\n",
    "\r\n",
    "- [detailed explanation-2](https://machinelearniowledge.ai/activation-functions-neural-network/)\r\n",
    "\r\n",
    "-\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# equation for the activation functions\r\n",
    "\r\n",
    "$$\r\n",
    "\r\n",
    "Logistic : 1/(1+ e^{-x}) \\\\\\\\\r\n",
    "Tanh     : (1 - e^{-2x})/(1+ e^{-2x}) \\\\\\\\\r\n",
    "RELU     : max(0, x) \\\\\\\\\r\n",
    "Leaky RELU : max(\\alpha * x, 0) \\\\\\\\\r\n",
    "Parametric RELU : same\\ as\\ leaky\\ RELU\\, but \\alpha\\ is\\ a\\ trainiable\\ parameter\\ , not\\ hyper-parameter \\\\\\\\\r\n",
    "ELU    :  for\\ x>0\\, x\\ and \\alpha * (e ^{x}-1)\\ for\\ x<=0 \\\\\r\n",
    "Scaled ELU :  for\\ x>0\\, scale * x \\ and\\ scale * \\alpha * (e ^{x}-1)\\ for\\ x<=0\r\n",
    "\r\n",
    "\r\n",
    "$$\r\n",
    "\r\n",
    "![Image](https://miro.medium.com/max/700/1*p_hyqAtyI8pbt2kEl6siOQ.png)\r\n",
    "\r\n",
    "![Activation image figure](https://assets.website-files.com/5d7b77b063a9066d83e1209c/60d251e885bcde1f45aee375_infographic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
