{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "884de052-358b-4ced-b511-60116be31d73",
   "metadata": {},
   "source": [
    "# Optimizers in Deep learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1de77c-0816-4e26-9058-91e7d37861de",
   "metadata": {},
   "source": [
    "\n",
    "## Optimizers\n",
    "\n",
    "- [List of optimizers](https://ruder.io/optimizing-gradient-descent/)\n",
    "- [Summary 1](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)\n",
    "- [Summary 2](https://mlfromscratch.com/optimizers-explained/#/)\n",
    "- [Summary 3](https://deeplearningdemystified.com/article/fdl-4)\n",
    "- [Equations](https://medium.datadriveninvestor.com/overview-of-different-optimizers-for-neural-networks-e0ed119440c3)\n",
    "- [Article 1](https://blog.paperspace.com/optimization-in-deep-learning/)\n",
    "- [ADAM](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "- [Exploring optimizers](https://heartbeat.fritz.ai/exploring-optimizers-in-machine-learning-7f18d94cd65b)\n",
    "\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "- [Article 1](https://mlfromscratch.com/neural-networks-explained/#backpropagation)\n",
    "- [article 2](https://www.jeremyjordan.me/neural-networks-training/)\n",
    "- [an example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
    "- [Implement backprop algo](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/)\n",
    "\n",
    "\n",
    "## Extras\n",
    "- [Saddle Point](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/maximums-minimums-and-saddle-points) and [2](https://datascience.stackexchange.com/a/24821/94070)\n",
    "            \n",
    "            A saddle point is a point on a function that is a stationary point but is not a local extremum. Also called minimax points, saddle points are typically observed on surfaces in three‚Äêdimensional space but also occur in lower or higher dimensions. The first and second derivative tests can often be used to distinguish between saddle points and other types of stationary points, such as local minima and maxima.\n",
    "            \n",
    "- [Vanishing gradient problem](https://stats.stackexchange.com/a/493347/279934)\n",
    "- [Preventing vanishing gradient problem](https://datascience.stackexchange.com/a/72352/94070)\n",
    "- [Vanishing and exploding Gradient problem](https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547efa8-b415-4312-9f9b-074ebb0e9620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
